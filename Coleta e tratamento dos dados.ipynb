{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38050451",
   "metadata": {},
   "source": [
    "# <span>Coletando e tratando os dados</span>\n",
    "<hr style=\"border:2px solid #0077b9;\">\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: center;font-size: 90%;\">\n",
    "    Autor: Flávio Belizário da Silva Mota\n",
    "    <br/><br/>\n",
    "    Instituto Nacional de Pesquisas Espaciais (INPE)\n",
    "    <br/>\n",
    "    Avenida dos Astronautas, 1758, Jardim da Granja, São José dos Campos, SP 12227-010, Brazil\n",
    "    <br/><br/>\n",
    "    Contato: <a href=\"mailto:flavio.belizario.mota@gmail.com\">flavio.belizario.mota@gmail.com</a>\n",
    "    <br/>\n",
    "    Professor: Rafael Santos\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: justify;  margin-left: 25%; margin-right: 25%;\">\n",
    "<b>Objetivo.</b> Esse caderno Jupyter tem como objetivo apresentar a etapa de coleta dos dados para o projeto da disciplina CAP394 - Introdução à Ciência de Dados. Os dados são provenientes do projeto CRISIS NLP e foram coletados utilizando publicações do Twitter referentes à pandemia de COVID-19. O conjunto contém informações geolocalizadas a respeito da origem da publicação e, se no corpo da publicação existem referências a outros locais, também são apresentadas as informações sobre esses locais. Esses dados são disponibilizados em formato json e podem ser baixados em arquivos que representam as publicações diárias. Nessa etapa de coleta, foram consideradas publicações feitas no Brasil.\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "<div style=\"text-align: center;font-size: 90%;\">\n",
    "    <b>Os dados podem ser encontrados em:</b>\n",
    "    <div style=\"margin-left: 10px; margin-right: 10px\">\n",
    "    https://crisisnlp.qcri.org/covid19\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a51098",
   "metadata": {},
   "source": [
    "## O conjunto de dados\n",
    "<hr style=\"border:2px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ead2a",
   "metadata": {},
   "source": [
    "<div>O conjunto de dados, chamado de GeoCoV19, foi elaborado através da coleta de dados do Twitter e contém mais de 524 milhões de <i>tweets</i> multilíngues coletados até 1º de maio de 2020. O conjunto contém cerca de 378 mil tweets com <i>geotags</i> e 5,4 milhões de <i>tweets</i> com informações sobre o local. Foram extraídos topônimos a partir do campo de localização do usuário e do conteúdo do <i>tweet</i> e essa informação foi transformada para geolocalizações, como país, estado ou cidade. Utilizando essa abordagem, 297 milhões de <i>tweets</i> foram associados a geolocalização usando o campo de localização do usuário e 452 milhões de <i>tweets</i> usando o conteúdo do <i>tweet</i>.</div>\n",
    "\n",
    "<div>Os dados com informações geolocalizadas estão disponíveis para <i>download</i> em <a>https://crisisnlp.qcri.org/covid19</a> no formato <i>.zip</i>.\n",
    "\n",
    "<figure>\n",
    "    <img src='images/geo-download.png' style='display: block; margin-left: auto; margin-right: auto;'/>\n",
    "    <figcaption style='text-align: center;'>Figura 1 - Seção de download dos arquivos.</figcaption>\n",
    "</figure> <br/>\n",
    "\n",
    "Cada arquivo <i>.zip</i> contém um JSON com os <i>tweets</i> de um dia. Para coletar todas as publicações, é necessário baixar cada arquivo individualmente, por dia.</div>\n",
    "    \n",
    "<figure>\n",
    "    <img src='images/geo-download-datas.png' style='max-width:400px; max-height:400px; width: auto; height: auto; display: block; margin-left: auto; margin-right: auto;'/>\n",
    "    <figcaption style='text-align: center;'>Figura 2 - Seleção do arquivo com tweets do dia.</figcaption>\n",
    "</figure> <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65705fe2",
   "metadata": {},
   "source": [
    "## Automatizando a coleta dos dados\n",
    "<hr style=\"border:2px solid #0077b9;\">\n",
    "\n",
    "<div>Realizar a coleta dessas publicações manualmente seria extremamente trabalhoso. Sendo assim, utilizaremos os códigos a seguir para automatizar essa coleta, construindo ao final um único arquivo JSON com as publicações do Brasil.</div>\n",
    "<br/>\n",
    "<div> <b>Mas de que forma faremos isso, se é preciso selecionar o arquivo que queremos baixar?</b> \n",
    "<br/> Inspecionando a página do projeto é possível descobrir que ao selecionar o botão de <i>download</i>, uma função é executada:\n",
    "</div>\n",
    "<br/>\n",
    "<figure>\n",
    "    <img src=\"images/insp-geo-download.png\" style='max-width:700px; max-height:600px; width: auto; height: auto; display: block; margin-left: auto; margin-right: auto;'/>\n",
    "    <figcaption style='text-align: center;'>Figura 3 - Inspecionando o botão de download.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "    Descobrimos que a função <code>geo_pass_selected()</code> é chamada quando selecionamos o arquivo que queremos baixar. Inspecionando os arquivos fonte da página, conseguimos identificar o que essa função faz:\n",
    "</div>\n",
    "<br/>\n",
    "<figure>\n",
    "    <img src=\"images/insp-geo-pass.png\" style='max-width:700px; max-height:600px; width: auto; height: auto; display: block; margin-left: auto; margin-right: auto;'/>\n",
    "    <figcaption style='text-align: center;'>Figura 4 - Identificando a função <code>geo_pass_selected()</code>.</figcaption>\n",
    "</figure>\n",
    "<br/>\n",
    "<div>\n",
    "    Podemos notar que a função constrói um nome para o arquivo a ser baixado concatenando \"geo_\" e \".zip\" a um valor que é obtido do elemento de seleção da página. Podemos inspecionar esse elemento para entender o que são esses valores:\n",
    "</div>\n",
    "<br/>\n",
    "<figure>\n",
    "    <img src=\"images/insp-geo-dates.png\" style='max-width:900px; max-height:800px; width: auto; height: auto; display: block; margin-left: auto; margin-right: auto;'/>\n",
    "    <figcaption style='text-align: center;'>Figura 5 - Inspecionando o componente de seleção do arquivo.</figcaption>\n",
    "</figure>\n",
    "<br/>\n",
    "<div>\n",
    "    Analisando o código javascript acima, podemos notar que os valores utilizados para preencher as opções do componente de seleção são gerados através de um laço de repetição que itera de uma data de início (01/02/2020) até uma data de fim (01/05/2020). Dessa forma, o nome do arquivo a ser baixado tem o formato \"geo_2020-02-01.zip\", por exemplo. A função, em seguida, concatena uma <i>url</i> <code>/covid_data/geo_files/</code> ao nome do arquivo para então disponibilizar o <i>download</i>. O arquivo baixado é um <i>.zip</i>, sendo necessário extrair seu conteúdo, para poder manipular o arquivo JSON contendo as publicações do dia.\n",
    "</div>\n",
    "<br/>\n",
    "<div>\n",
    "    Analisando o número de publicações diárias na imagem a seguir, é possível notar um aumento a partir do dia 21 de fevereiro de 2020 e um segundo aumento a partir do dia 9 de março de 2020. Oficialmente, a Organização Mundial da Saúde (OMS) declarou estado de pandemia do novo Coronavírus em 11 de março de 2020 (<a href=\"https://g1.globo.com/bemestar/coronavirus/noticia/2020/03/11/oms-declara-pandemia-de-coronavirus.ghtml\">G1</a>), onde também é possível notar um aumento no número de publicações.\n",
    "</div>\n",
    "<br/>\n",
    "<figure>\n",
    "    <img src=\"images/daily_tweet_distribution.png\" style='max-width:900px; max-height:800px; width: auto; height: auto; display: block; margin-left: auto; margin-right: auto;'/>\n",
    "    <figcaption style='text-align: center;'>Figura 6 - Distribuição diária de publicações entre 1º de fevereiro e 1º de maio de 2020.</figcaption>\n",
    "</figure>\n",
    "<br/>\n",
    "<div>\n",
    "    Considerando o volume de dados no intervalo, visando tornar o processamento e análise processos menos custosos em termos de tempo e recurso, serão coletadas as publicações entre os dias 19 de fevereiro e 20 de março de 2020, contemplando assim um intervalo de 30 dias com publicações anteriores e posteriores ao decreto da OMS. \n",
    "</div>\n",
    "<br/>\n",
    "<div>\n",
    "    De posse das informações apresentadas anteriormente, a automatização do processo de coleta dos dados pode ser descrito como as seguintes etapas:\n",
    "    <br/>\n",
    "    <ul>\n",
    "        <li>Criação de uma lista de datas no intervalo 19/02/2020 até 20/03/2020;</li>\n",
    "        <li>Iterar pelas datas dessa lista construindo o nome e <i>url</i> do arquivo a ser baixado;</li>\n",
    "        <li>Realizar o <i>download</i> do arquivo;</li>\n",
    "        <li>Extrair o conteúdo do arquivo <i>.zip</i> baixado;</li>\n",
    "        <li>Analisar o arquivo JSON buscando pelas publicações feitas no Brasil;</li>\n",
    "        <li>Gerar um novo arquivo JSON com essas publicações;</li>\n",
    "        <li>Apagar o arquivo baixado para que ele não ocupe muito espaço em disco (considerando que os arquivos em sua maioria ocupam mais de 100 MB).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788b0dd",
   "metadata": {},
   "source": [
    "### Instalando e importando as bibliotecas\n",
    "<hr style=\"border:1px solid #0077b9;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb6eba4",
   "metadata": {},
   "source": [
    "Instalando a biblioteca <code>wget</code> para realizar o download dos arquivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379aa2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\flavi\\anaconda3\\lib\\site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567bd27",
   "metadata": {},
   "source": [
    "Instalando a biblioteca <code>zipfile</code> para manipular os arquivos .zip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74dd0e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipfile36 in c:\\users\\flavi\\anaconda3\\lib\\site-packages (0.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install zipfile36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a196b",
   "metadata": {},
   "source": [
    "Importando as bibliotecas necessárias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3c5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget # biblioteca para download de arquivos\n",
    "import json # biblioteca para manipulação de JSON\n",
    "import pandas as pd # biblioteca para manipulação dos dados\n",
    "import os # biblioteca para acessar recursos do sistema operacional\n",
    "import zipfile # biblioteca para manipular arquivos .zip\n",
    "from datetime import datetime # bibilioteca para manipular datas\n",
    "import time # biblioteca para monitorar o tempo de execução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8409a51f",
   "metadata": {},
   "source": [
    "### Fluxo para carregamento dos dados\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f3a81",
   "metadata": {},
   "source": [
    "Primeiro, vamos definir o intervalo de datas que será coletado. Para isso podemos empregar a bilblioteca <code>pandas</code>. As datas contemplam o intervalo de tempo entre 19 de fevereiro de 2020 e 20 de março de 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f70dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inicio = datetime.strptime(\"2020-02-19\", \"%Y-%m-%d\")\n",
    "fim = datetime.strptime(\"2020-03-20\", \"%Y-%m-%d\")\n",
    "datas_geradas = pd.date_range(inicio, fim) # gerando o intervalo\n",
    "datas = datas_geradas.strftime(\"%Y-%m-%d\").values # obtendo uma lista com as datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4c8368",
   "metadata": {},
   "source": [
    "Agora, iterando pela lista de datas, os arquivos correspondentes serão baixados empregando a biblioteca <code>wget</code>. Como estão em formato <i>.zip</i>, será necessário extrair o conteúdo desses arquivos. Utilizaremos a biblioteca <code>zipfile</code> para isso. Após a extração, é possível iterar pelas linhas do arquivo, obtendo o JSON correspondente de cada uma das linhas.\n",
    "</br></br>\n",
    "Na estrutura do JSON existem 8 chaves, como descrito na documentação dos dados, disponível <a href=\"https://crisisnlp.qcri.org/covid_data/geocov19_readme.txt\">aqui</a>. As chaves são:\n",
    "<ul>\n",
    "    <li>tweet_id: o identificador único de um <i>tweet</i>;</li>\n",
    "    <li>created_at: data e hora da criação do <i>tweet</i>, no formato UTC;</li>\n",
    "    <li>user_id: identificador único do usuário que publicou o <i>tweet</i>;</li>\n",
    "    <li>geo_source: este campo pode ter um dos quatro valores: (i) <code>coordinates</code>, (ii) <code>place</code>, (iii) <code>user_location</code> ou (iv) <code>tweet_text</code>. O valor depende da disponibilidade desses campos. No entanto, a prioridade é dada aos campos mais precisos, quando disponíveis. A ordem de prioridade é <code>coordinates</code>, <code>place</code>, <code>user_location</code> e <code>tweet_text</code>. Por exemplo, quando um <i>tweet</i> tem coordenadas de GPS, o valor será \"coordinates\", mesmo que todos os outros campos de localização estejam presentes. Se um <i>tweet</i> não tiver informações de GPS, place (local) e user_location (localização informada pelo usuário), o valor desse campo será \"tweet_text\" se houver alguma menção a localidades no texto do <i>tweet</i>.;</li>\n",
    "</ul>\n",
    "\n",
    "As demais chaves podem ser preenchidas com uma estrutura chamada <code>location_json</code>. Um exemplo dessa estrutura pode ser o seguinte:\n",
    "<code>location_json: {\"country_code\":\"us\",\"state\":\"California\",\"county\":\"San Francisco\",\"city\":\"San Francisco\"}</code>.\n",
    "Dependendo da granularidade dos dados disponíveis, as chaves <code>country_code, state, county ou city</code> podem estar ausentes no <code>location_json</code>.\n",
    "\n",
    "<ul>\n",
    "    <li>user_location: pode conter um \"location_json\" conforme descrito acima ou um JSON vazio. Este campo usa os metadados da informação \"local\" do perfil de um usuário do Twitter e representa a localização declarada do usuário no formato de texto;</li>\n",
    "    <li>geo: pode conter um \"location_json\" conforme descrito acima ou um JSON vazio. Representa o campo \"geo\" informado pelo Twitter;</li>\n",
    "    <li>tweet_locations: este campo pode conter uma matriz de \"location_json\" conforme descrito acima (<code>[location_json1, location_json2]</code>) ou uma matriz vazia. Este campo usa o conteúdo do <i>tweet</i> (ou seja, a publicação real) para encontrar topônimos. Uma publicação pode ter várias menções a locais diferentes (ou seja, topônimos). É por isso que existe uma variedade de locais representando todos esses topônimos em um <i>tweet</i>. Por exemplo, em um <i>tweet</i> como \"O Reino Unido tem mais de 65.000 mortes por #COVID19. Mais do que Qatar, Paquistão e Noruega.\", há quatro menções de localização. A matriz <code>tweet_locations</code> deve representar esses quatro separadamente;</li>\n",
    "    <li>place: este campo pode conter um \"location_json\" como descrito acima ou um JSON vazio. Ele representa o campo \"lugar\" fornecido pelo Twitter.</li>\n",
    "</ul>\n",
    "\n",
    "Considerando as informações acima, a estratégia adotada para coletar os dados foi encontrar em cada JSON aqueles que possuem o campo <code>geo_source</code> com os valores <code>coordinates</code>, <code>place</code> ou <code>user_location</code>, pois são os valores que indicam com maior precisão a localização de origem da publicação. Em seguida, consideramos o campo <code>user_location</code>. Caso esse campo não seja vazio, procuramos por uma chave <code>country_code</code> que contenha o valor br, uma vez que queremos publicações cujo país de origem seja o Brasil.\n",
    "<br/><br/>\n",
    "Por fim, adicionamos o JSON que atende às especificações ao conjunto de dados final e excluímos o arquivo que estava sendo analisado, para que um novo possa ser baixado, extraído e analisado, até que todas as datas da lista sejam obtidas. Vamos registrar o tempo necessário para executar todo esse processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a2fc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando coleta em 12/08/2022 - 14:53:14\n",
      "Baixando arquivo: geo_2020-02-19.zip\n",
      "Duração 26.33 segundos\n",
      "Duração da coleta dos dados do JSON 17.73 segundos\n",
      "Removendo arquivo geo_2020-02-19.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-02-20.zip\n",
      "Duração 14.25 segundos\n",
      "Duração da coleta dos dados do JSON 13.65 segundos\n",
      "Removendo arquivo geo_2020-02-20.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-02-21.zip\n",
      "Duração 28.77 segundos\n",
      "Duração da coleta dos dados do JSON 20.52 segundos\n",
      "Removendo arquivo geo_2020-02-21.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-02-22.zip\n",
      "Duração 29.16 segundos\n",
      "Duração da coleta dos dados do JSON 25.49 segundos\n",
      "Removendo arquivo geo_2020-02-22.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-02-23.zip\n",
      "Duração 34.01 segundos\n",
      "Duração da coleta dos dados do JSON 29.54 segundos\n",
      "Removendo arquivo geo_2020-02-23.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-02-24.zip\n",
      "Duração 44.76 segundos\n",
      "Duração da coleta dos dados do JSON 48.89 segundos\n",
      "Removendo arquivo geo_2020-02-24.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-02-25.zip\n",
      "Duração 75.23 segundos\n",
      "Duração da coleta dos dados do JSON 69.07 segundos\n",
      "Removendo arquivo geo_2020-02-25.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-02-26.zip\n",
      "Duração 67.59 segundos\n",
      "Duração da coleta dos dados do JSON 101.08 segundos\n",
      "Removendo arquivo geo_2020-02-26.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-02-27.zip\n",
      "Duração 69.54 segundos\n",
      "Duração da coleta dos dados do JSON 108.65 segundos\n",
      "Removendo arquivo geo_2020-02-27.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-02-28.zip\n",
      "Duração 71.04 segundos\n",
      "Duração da coleta dos dados do JSON 90.57 segundos\n",
      "Removendo arquivo geo_2020-02-28.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-02-29.zip\n",
      "Duração 66.59 segundos\n",
      "Duração da coleta dos dados do JSON 79.27 segundos\n",
      "Removendo arquivo geo_2020-02-29.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-01.zip\n",
      "Duração 78.52 segundos\n",
      "Duração da coleta dos dados do JSON 73.15 segundos\n",
      "Removendo arquivo geo_2020-03-01.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-02.zip\n",
      "Duração 58.38 segundos\n",
      "Duração da coleta dos dados do JSON 72.71 segundos\n",
      "Removendo arquivo geo_2020-03-02.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-03.zip\n",
      "Duração 63.56 segundos\n",
      "Duração da coleta dos dados do JSON 68.39 segundos\n",
      "Removendo arquivo geo_2020-03-03.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-04.zip\n",
      "Duração 52.59 segundos\n",
      "Duração da coleta dos dados do JSON 75.65 segundos\n",
      "Removendo arquivo geo_2020-03-04.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-05.zip\n",
      "Duração 60.61 segundos\n",
      "Duração da coleta dos dados do JSON 79.94 segundos\n",
      "Removendo arquivo geo_2020-03-05.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-06.zip\n",
      "Duração 69.42 segundos\n",
      "Duração da coleta dos dados do JSON 78.70 segundos\n",
      "Removendo arquivo geo_2020-03-06.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-07.zip\n",
      "Duração 78.11 segundos\n",
      "Duração da coleta dos dados do JSON 95.55 segundos\n",
      "Removendo arquivo geo_2020-03-07.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-08.zip\n",
      "Duração 80.37 segundos\n",
      "Duração da coleta dos dados do JSON 95.81 segundos\n",
      "Removendo arquivo geo_2020-03-08.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-09.zip\n",
      "Duração 74.91 segundos\n",
      "Duração da coleta dos dados do JSON 113.45 segundos\n",
      "Removendo arquivo geo_2020-03-09.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-10.zip\n",
      "Duração 83.43 segundos\n",
      "Duração da coleta dos dados do JSON 99.77 segundos\n",
      "Removendo arquivo geo_2020-03-10.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-11.zip\n",
      "Duração 91.34 segundos\n",
      "Duração da coleta dos dados do JSON 100.68 segundos\n",
      "Removendo arquivo geo_2020-03-11.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-12.zip\n",
      "Duração 64.76 segundos\n",
      "Duração da coleta dos dados do JSON 132.21 segundos\n",
      "Removendo arquivo geo_2020-03-12.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-13.zip\n",
      "Duração 136.20 segundos\n",
      "Duração da coleta dos dados do JSON 140.36 segundos\n",
      "Removendo arquivo geo_2020-03-13.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-14.zip\n",
      "Duração 109.49 segundos\n",
      "Duração da coleta dos dados do JSON 132.85 segundos\n",
      "Removendo arquivo geo_2020-03-14.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-15.zip\n",
      "Duração 113.78 segundos\n",
      "Duração da coleta dos dados do JSON 146.83 segundos\n",
      "Removendo arquivo geo_2020-03-15.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-16.zip\n",
      "Duração 106.04 segundos\n",
      "Duração da coleta dos dados do JSON 143.49 segundos\n",
      "Removendo arquivo geo_2020-03-16.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-17.zip\n",
      "Duração 91.31 segundos\n",
      "Duração da coleta dos dados do JSON 205.32 segundos\n",
      "Removendo arquivo geo_2020-03-17.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-18.zip\n",
      "Duração 91.19 segundos\n",
      "Duração da coleta dos dados do JSON 134.15 segundos\n",
      "Removendo arquivo geo_2020-03-18.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-19.zip\n",
      "Duração 99.39 segundos\n",
      "Duração da coleta dos dados do JSON 109.64 segundos\n",
      "Removendo arquivo geo_2020-03-19.zip\n",
      "---------------------------------\n",
      "Baixando arquivo: geo_2020-03-20.zip\n",
      "Duração 56.68 segundos\n",
      "Duração da coleta dos dados do JSON 101.64 segundos\n",
      "Removendo arquivo geo_2020-03-20.zip\n",
      "---------------------------------\n",
      "Duração total da coleta 83.32 minutos\n",
      "Coleta finalizada em 12/08/2022 - 16:16:33\n"
     ]
    }
   ],
   "source": [
    "print('Iniciando coleta em %s' % (datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\")))\n",
    "tac_geral = time.time()\n",
    "for data in datas:\n",
    "    json_final = []\n",
    "    doc = \"geo_\"+ data +\".zip\"\n",
    "    arq = \"geo_\"+ data +\".json\"\n",
    "    doc_url = \"https://crisisnlp.qcri.org/covid_data/geo_files/\"+ doc\n",
    "    print('Baixando arquivo: %s' % doc)\n",
    "    tac = time.time()\n",
    "    wget.download(doc_url)\n",
    "    tic = time.time()\n",
    "    print(\"Duração %.2f segundos\" % (tic - tac))\n",
    "    \n",
    "    tac = time.time()\n",
    "    with zipfile.ZipFile(doc, mode=\"r\") as arquivo:\n",
    "        with arquivo.open(arq, mode=\"r\") as arq_json:\n",
    "            for linha in arq_json:\n",
    "                j_linha = json.loads(linha)\n",
    "                if j_linha['geo_source'] == 'coordinates' or j_linha['geo_source'] == 'place' or j_linha['geo_source'] == 'user_location':\n",
    "                    if len(j_linha['user_location']) != 0 and j_linha['user_location']['country_code'] == 'br': # Aqui definimos que queremos publicações de origem no Brasil\n",
    "                        json_final.append(j_linha)\n",
    "            arq_json.close()\n",
    "        arquivo.close()\n",
    "    \n",
    "    json_nome = \"geo_json_\" + data + '.json'\n",
    "    caminho_arquivo = os.path.join('dados/geo_' + data, json_nome)\n",
    "    if not os.path.exists('dados/geo_' + data):\n",
    "        os.makedirs('dados/geo_' + data)\n",
    "    \n",
    "    with open(caminho_arquivo, \"w\") as arquivo_json:\n",
    "        json.dump(json_final, arquivo_json)\n",
    "        arquivo_json.close()\n",
    "    \n",
    "    tic = time.time()    \n",
    "    print(\"Duração da coleta dos dados do JSON %.2f segundos\" % (tic - tac))\n",
    "    \n",
    "    if os.path.isfile(doc):\n",
    "        os.remove(doc)\n",
    "        print(\"Removendo arquivo %s\" % doc)\n",
    "    else: \n",
    "        print(\"Erro: %s - arquivo não encontrado\" % doc)\n",
    "    \n",
    "    print('---------------------------------')\n",
    "    \n",
    "tic_geral = time.time()\n",
    "print(\"Duração total da coleta %.2f minutos\" % ((tic_geral - tac_geral)/60))\n",
    "print(\"Coleta finalizada em %s\" % (datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e0795-4d8a-4f53-bedf-89a156bb7ec1",
   "metadata": {},
   "source": [
    "O processo de coleta levou aproximadamente 1 hora e 24 minutos.\n",
    "<br/>\n",
    "Vamos criar funções que ajudem a contabilizar o tamanho do conjunto de dados gerados ao final dessa etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40672955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.40GB'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retorna_tamanho_formatado(b, fator=1024, sufixo=\"B\"):\n",
    "    for unidade in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\"]:\n",
    "        if b < fator:\n",
    "            return f\"{b:.2f}{unidade}{sufixo}\"\n",
    "        b /= fator\n",
    "    return f\"{b:.2f}Y{sufixo}\"\n",
    "\n",
    "def retorna_tamanho_diretorio(diretorio):\n",
    "    total = 0\n",
    "    try:\n",
    "        for elemento in os.scandir(diretorio):\n",
    "            if elemento.is_file():\n",
    "                total += elemento.stat().st_size\n",
    "            elif elemento.is_dir():\n",
    "                try:\n",
    "                    total += retorna_tamanho_diretorio(elemento.path)\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "    except NotADirectoryError:\n",
    "        return os.path.getsize(diretorio)\n",
    "    except PermissionError:\n",
    "        return 0\n",
    "    return total\n",
    "     \n",
    "retorna_tamanho_formatado(retorna_tamanho_diretorio(\"dados\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee72cb-2686-4402-b06a-ba9539cff2c0",
   "metadata": {},
   "source": [
    "São 1.4GB de dados coletados nesse intervalo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764315f6-9892-4858-8403-0c7f7d6d797a",
   "metadata": {},
   "source": [
    "### Tratando os dados coletados\n",
    "<hr style=\"border:1px solid #0077b9;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f63a6b-2d7e-47af-a907-2e7701a7dac2",
   "metadata": {},
   "source": [
    "Após a coleta dos dados, o que temos é um conjunto de pastas contendo arquivos JSON com as publicações feitas entre as datas informadas (19/02/2020 e 20/03/2020) e de origem do Brasil. \n",
    "<br><br>\n",
    "Vamos concatenar esses arquivos em um único arquivo CSV, para facilitar o carregamento e manipulação dos dados. Para isso, primeiramente vamos definir uma função que vai ser utilizada para a criação de dois campos no conjunto de dados: estado e cidade. Essa função vai iterar pelos dados e extrair do JSON <code>user_location</code> o nome do estado e da cidade de origem da publicação, quando houver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a471309c-e3ac-4d7f-80f8-79197e96f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separa_estado_cidade(user_location):\n",
    "    estados = [None] * len(user_location)\n",
    "    cidades = [None] * len(user_location)\n",
    "    for index, item in user_location.items():\n",
    "        for k, v in item.items():\n",
    "            if k == 'state': # quando encontrar uma chave de estado, adiciona ao vetor com os estados\n",
    "                estados[index] = v\n",
    "            if k == 'city': # quando encontrar uma chave de cidade, adiciona ao vetor com as cidades\n",
    "                cidades[index] = v\n",
    "                \n",
    "    return (estados, cidades)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3948758c-64bb-451a-8698-2729f965dcaa",
   "metadata": {},
   "source": [
    "Após a criação da função, é possível iterar pelas pastas e arquivos que foram coletados, removendo do conjunto os campos <code>tweet_id, user_id, geo, place</code> e <code>tweet_locations</code> e gerando os atributos <code>estado</code> e <code>cidade</code>. O arquivo gerado ao final desse tratamento é um CSV com os campos <code>created_at, geo_source, estado</code> e <code>cidade</code>. Vamos registrar o tempo de execução do tratamento desses dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a30bc64-896a-48c6-9a6c-00146bfb5a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando tratamento dos dados em 12/08/2022 - 21:00:23\n",
      "Lendo arquivo geo_json_2020-02-19.json\n",
      "Arquivo com 5240 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-02-20.json\n",
      "Arquivo com 4002 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-02-21.json\n",
      "Arquivo com 5396 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-02-22.json\n",
      "Arquivo com 5599 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-02-23.json\n",
      "Arquivo com 7697 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-02-24.json\n",
      "Arquivo com 13415 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-02-25.json\n",
      "Arquivo com 21280 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-02-26.json\n",
      "Arquivo com 101251 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-02-27.json\n",
      "Arquivo com 88302 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-02-28.json\n",
      "Arquivo com 59678 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-02-29.json\n",
      "Arquivo com 59791 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-01.json\n",
      "Arquivo com 67834 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-02.json\n",
      "Arquivo com 39231 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-03.json\n",
      "Arquivo com 27195 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-04.json\n",
      "Arquivo com 29615 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-05.json\n",
      "Arquivo com 28444 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-06.json\n",
      "Arquivo com 30343 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-07.json\n",
      "Arquivo com 30380 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-08.json\n",
      "Arquivo com 24577 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-09.json\n",
      "Arquivo com 31946 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-10.json\n",
      "Arquivo com 38085 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-11.json\n",
      "Arquivo com 48017 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-12.json\n",
      "Arquivo com 102363 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-13.json\n",
      "Arquivo com 152501 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-14.json\n",
      "Arquivo com 105891 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-15.json\n",
      "Arquivo com 120376 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-16.json\n",
      "Arquivo com 136010 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-17.json\n",
      "Arquivo com 173003 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-18.json\n",
      "Arquivo com 121430 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-19.json\n",
      "Arquivo com 103745 registros\n",
      "---------------------------------\n",
      "Lendo arquivo geo_json_2020-03-20.json\n",
      "Arquivo com 80735 registros\n",
      "---------------------------------\n",
      "Duração total da coleta 6.89 minutos\n",
      "Tratamento finalizado em 12/08/2022 - 21:07:17\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['created_at', 'geo_source', 'estado', 'cidade'])\n",
    "df.to_csv('tweets_covid_brasil.csv', index=False)\n",
    "print('Iniciando tratamento dos dados em %s' % (datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\")))\n",
    "tac_geral = time.time()\n",
    "for data in datas:\n",
    "    \n",
    "    json_nome = \"geo_json_\" + data + '.json'\n",
    "    print('Lendo arquivo %s' % json_nome)\n",
    "    caminho_arquivo = os.path.join('dados/geo_' + data, json_nome)\n",
    "    df = pd.read_json(caminho_arquivo)\n",
    "    print('Arquivo com %s registros' % df.shape[0])\n",
    "    df = df.drop(columns=['tweet_id', 'user_id', 'geo', 'place', 'tweet_locations'])\n",
    "    estados, cidades = separa_estado_cidade(df['user_location'])\n",
    "    df['estado'] = estados\n",
    "    df['cidade'] = cidades\n",
    "    df = df.drop(columns='user_location')\n",
    "    df.to_csv('tweets_covid_brasil.csv', mode='a', index=False, header=False)\n",
    "    print('---------------------------------')\n",
    "    \n",
    "tic_geral = time.time()\n",
    "print(\"Duração total do tratamento: %.2f minutos\" % ((tic_geral - tac_geral)/60))\n",
    "print(\"Tratamento finalizado em %s\" % (datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d147d7-32f8-4c34-ad95-a3bde5fb5e89",
   "metadata": {},
   "source": [
    "O tratamento levou cerca de 7 minutos. Podemos verificar o tamanho do arquivo CSV gerado (tweets_covid_brasil.csv): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffdf5d20-02cd-4007-9dd8-356367266042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'92.75MB'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retorna_tamanho_formatado(os.path.getsize('tweets_covid_brasil.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68a9bbe-c885-48b0-8549-57391356043d",
   "metadata": {},
   "source": [
    "Agora, ao invés de manipular 1.4GB de dados, podemos trabalhar com um arquivo relativamente menor.\n",
    "<br><br>\n",
    "Vamos verificar a quantidade de linhas que o arquivo final possui:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "648db7a7-8cb9-437f-836b-8ea51bdbc5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O arquivo final contém 1863372 linhas.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_covid_brasil.csv')\n",
    "print(\"O arquivo final contém %s linhas.\" % df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6192b130-2fbf-4765-bf68-39c5323439e2",
   "metadata": {},
   "source": [
    "Agora temos um arquivo para gerar nossas análises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f63914-f414-41d9-8525-33c4154e2232",
   "metadata": {},
   "source": [
    "<a href=\"/index\">Voltar</a>\n",
    "<br>\n",
    "<a href=\"/analise\">Avançar para Análise de dados</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
